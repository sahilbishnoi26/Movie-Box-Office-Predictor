{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/npop/Code/Projects/ML7641_Project/DLenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nbformat\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For BERTopic\n",
    "from bertopic import BERTopic\n",
    "import os\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Leet topic\n",
    "from leet_topic import leet_topic\n",
    "\n",
    "# For Topic Modeling Evaluation\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(\"/Users/npop/Code/Projects/ML7641_Project/Official_Datasets/stage_2/movie_data_valid.csv\")\n",
    "reddit_comments = pd.read_csv(\"/Users/npop/Code/Projects/ML7641_Project/Official_Datasets/stage_2/reddit_comments_valid.csv\")\n",
    "youtube_comments = pd.read_csv(\"/Users/npop/Code/Projects/ML7641_Project/Official_Datasets/stage_2/youtube_comments_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any potential duplicates that remain in the data frames\n",
    "reddit_comments.drop_duplicates(subset=['body', 'post_date'], inplace=True)\n",
    "# The reddit comments can't have any missing values in the following fields\n",
    "reddit_comments.dropna(subset=['body'], inplace=True)\n",
    "reddit_comments.dropna(subset=['post_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove comments that were deleted by the user or removed by the moderators\n",
    "deleted_comments = reddit_comments[reddit_comments['body'].str.contains('\\[deleted\\]|\\[removed\\]')].index.to_list()\n",
    "reddit_comments.drop(index=deleted_comments, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There were 9870 comments that had links in them. We will simply remove the links and keep the rest of the text\n",
    "\n",
    "url_pattern = r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "comments_with_links = reddit_comments.loc[reddit_comments['body'].str.contains(url_pattern)].index.to_list()\n",
    "\n",
    "def remove_links(reddit_comments=reddit_comments, ids=comments_with_links) -> None:\n",
    "    import re\n",
    "    \n",
    "    def sub_link(text:str) -> str:\n",
    "        comment_text = re.sub(url_pattern, \"\", text)\n",
    "        return comment_text\n",
    "    \n",
    "    reddit_comments.loc[ids, \"body\"] = reddit_comments.loc[comments_with_links, \"body\"].apply(sub_link)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_short = reddit_comments[reddit_comments['body'].str.len() < 3].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_comments.drop(index=too_short, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    # Create spacy object\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_comments = lemmatization(reddit_comments['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_comments.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236228 entries, 0 to 236227\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   body                236228 non-null  object \n",
      " 1   post_date           236228 non-null  object \n",
      " 2   upvotes             236228 non-null  float64\n",
      " 3   parent_id           236228 non-null  object \n",
      " 4   top_level_id        236228 non-null  object \n",
      " 5   post_title          236228 non-null  object \n",
      " 6   post_id             236228 non-null  object \n",
      " 7   movie_title         236228 non-null  object \n",
      " 8   movie_release_date  236228 non-null  object \n",
      " 9   movie_actors        236228 non-null  object \n",
      " 10  valid_post_date     236228 non-null  bool   \n",
      "dtypes: bool(1), float64(1), object(9)\n",
      "memory usage: 18.2+ MB\n"
     ]
    }
   ],
   "source": [
    "reddit_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_comments['parent_id'].fillna(\"Top Level\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_top_level = reddit_comments[reddit_comments['parent_id'].str.contains(\"Top Level\")].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_full = reddit_comments['body'].to_list() # Model requires documents to be in List[str] format\n",
    "comments_toplevel = reddit_top_level['body'].to_list()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" # To avoid warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:Use pytorch device_name: mps\n",
      "Batches: 100%|██████████| 7383/7383 [03:13<00:00, 38.08it/s] \n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(comments_full, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom UMAP & HBDScan Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, UMAP has stochastic behavior, set random_state for reproducibility\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='eom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', min_df=2, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Topic Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_rep = KeyBERTInspired()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    representation_model=key_rep,\n",
    "    top_n_words=10,\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(comments_full, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>130784</td>\n",
       "      <td>-1_character_movie_characters_bad</td>\n",
       "      <td>[character, movie, characters, bad, film, does...</td>\n",
       "      <td>[Have you not seen Celery Man?, It's not very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8516</td>\n",
       "      <td>0_teaser trailer_trailer trailer_watch trailer...</td>\n",
       "      <td>[teaser trailer, trailer trailer, watch traile...</td>\n",
       "      <td>[Yeah, and what about \"Teaser trailer\" ?, Was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4631</td>\n",
       "      <td>1_good actor_great actor_actor_director</td>\n",
       "      <td>[good actor, great actor, actor, director, act...</td>\n",
       "      <td>[Same director as Inside Out and Up so..., He'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4037</td>\n",
       "      <td>2_good actress_actress_actresses_cast</td>\n",
       "      <td>[good actress, actress, actresses, cast, actin...</td>\n",
       "      <td>[Implying Twilight had a bad leading actress? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4005</td>\n",
       "      <td>3_movie movie_movie_film_good movie</td>\n",
       "      <td>[movie movie, movie, film, good movie, films, ...</td>\n",
       "      <td>[In what movie?, I will see this movie ( ;) ) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3880</td>\n",
       "      <td>4_song trailer_trailer song_music trailer_soun...</td>\n",
       "      <td>[song trailer, trailer song, music trailer, so...</td>\n",
       "      <td>[What is the song in the trailer?, song?, \\n[L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>2740</td>\n",
       "      <td>5_downvoted_right_yeah right_downvote</td>\n",
       "      <td>[downvoted, right, yeah right, downvote, downv...</td>\n",
       "      <td>[He's definitely being downvoted, that was the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>2480</td>\n",
       "      <td>6_choo choo_choo_fookin prawns_ho ho</td>\n",
       "      <td>[choo choo, choo, fookin prawns, ho ho, ho, pr...</td>\n",
       "      <td>[Oh ho ho hohoho hoooooooooooooooooooooooooooo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>2295</td>\n",
       "      <td>7_book book_second book_book_book movie</td>\n",
       "      <td>[book book, second book, book, book movie, lov...</td>\n",
       "      <td>[The book!  You should read the book!, They sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>2166</td>\n",
       "      <td>8_shit pants_intensifies_pants_jeans</td>\n",
       "      <td>[shit pants, intensifies, pants, jeans, underw...</td>\n",
       "      <td>[I just shit my pants. , Me too.  They make me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>1932</td>\n",
       "      <td>9_release date_release_releases_released</td>\n",
       "      <td>[release date, release, releases, released, ja...</td>\n",
       "      <td>[Release date?, AND STILL NO RELEASE DATE!?!?!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>1799</td>\n",
       "      <td>10_vegan_gluten free_gluten_meat</td>\n",
       "      <td>[vegan, gluten free, gluten, meat, cheese, foo...</td>\n",
       "      <td>[[**Cheese**](https://m.youtube.com/watch?v=oM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>1781</td>\n",
       "      <td>11_horror film_horror movie_horror films_horro...</td>\n",
       "      <td>[horror film, horror movie, horror films, horr...</td>\n",
       "      <td>[This is very obviously not a horror movie. , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>1641</td>\n",
       "      <td>12_oscar bait_getting oscar_oscar baity_oscar</td>\n",
       "      <td>[oscar bait, getting oscar, oscar baity, oscar...</td>\n",
       "      <td>[Not everything has to be oscar bait. , That a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>1463</td>\n",
       "      <td>13_british accent_english accent_british accen...</td>\n",
       "      <td>[british accent, english accent, british accen...</td>\n",
       "      <td>[+British accent+ \"Obviously\", I... liked his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>1401</td>\n",
       "      <td>14_black people_white people_blacks_racists</td>\n",
       "      <td>[black people, white people, blacks, racists, ...</td>\n",
       "      <td>[Because black people, Black people, Please pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>1246</td>\n",
       "      <td>15_city_cities_filmed_town</td>\n",
       "      <td>[city, cities, filmed, town, downtown, movie f...</td>\n",
       "      <td>[The city?, And a city., Filmed in Tampere! Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>1210</td>\n",
       "      <td>16_money_money money_dollars_cash</td>\n",
       "      <td>[money, money money, dollars, cash, make money...</td>\n",
       "      <td>[# TAKE MY MONEY!!!!!!!!!!!, Money, Money. ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>1049</td>\n",
       "      <td>17_excited wait_looking forward_im excited_wai...</td>\n",
       "      <td>[excited wait, looking forward, im excited, wa...</td>\n",
       "      <td>[Can't wait for this. , Now I can't wait!!, I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>1048</td>\n",
       "      <td>18_harry potter_potter movies_potter_harry</td>\n",
       "      <td>[harry potter, potter movies, potter, harry, h...</td>\n",
       "      <td>[*Fantastic Beasts and Where to Find Them: At ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>1013</td>\n",
       "      <td>19_blue_orange blue_blue orange_colors</td>\n",
       "      <td>[blue, orange blue, blue orange, colors, colou...</td>\n",
       "      <td>[Cinematography., Blue is the most human color...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>999</td>\n",
       "      <td>20_gonna watch_worth watching_watch_ll watch</td>\n",
       "      <td>[gonna watch, worth watching, watch, ll watch,...</td>\n",
       "      <td>[I must watch this., I’d watch., I'd watch it.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>951</td>\n",
       "      <td>21_second season_season season_seasons_episode...</td>\n",
       "      <td>[second season, season season, seasons, episod...</td>\n",
       "      <td>[Six movies and a season., Where is season 2?,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>936</td>\n",
       "      <td>22_cgi_better cgi_cgi really_just cgi</td>\n",
       "      <td>[cgi, better cgi, cgi really, just cgi, cgi ju...</td>\n",
       "      <td>[Too much CGI now :(, Too much CGI, CGI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>899</td>\n",
       "      <td>23_cast_cast cast_cast really_cast just</td>\n",
       "      <td>[cast, cast cast, cast really, cast just, good...</td>\n",
       "      <td>[How would you of cast it?, What a cast., That...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>867</td>\n",
       "      <td>24_killer whales_shark_whales_whale</td>\n",
       "      <td>[killer whales, shark, whales, whale, predator...</td>\n",
       "      <td>[Predator Reddit. , Throwaway, because I spoke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>840</td>\n",
       "      <td>25_american sniper_sniper_soldier_war movie</td>\n",
       "      <td>[american sniper, sniper, soldier, war movie, ...</td>\n",
       "      <td>[The thing is that the \"heartland\" is anti-war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>805</td>\n",
       "      <td>26_dog dog_dog_dog movie_dog scene</td>\n",
       "      <td>[dog dog, dog, dog movie, dog scene, dog die, ...</td>\n",
       "      <td>[Yes? This is dog., The Dog and the Slumulous,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>751</td>\n",
       "      <td>27_gif oh_gif_gif gif_giphy gif</td>\n",
       "      <td>[gif oh, gif, gif gif, giphy gif, gif just, co...</td>\n",
       "      <td>[http://i.imgur.com/zaF6b0H.gif, http://i.imgu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>742</td>\n",
       "      <td>28_video games_video game_game movies_games movie</td>\n",
       "      <td>[video games, video game, game movies, games m...</td>\n",
       "      <td>[For people that do not play video games. My w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic   Count                                               Name  \\\n",
       "0      -1  130784                  -1_character_movie_characters_bad   \n",
       "1       0    8516  0_teaser trailer_trailer trailer_watch trailer...   \n",
       "2       1    4631            1_good actor_great actor_actor_director   \n",
       "3       2    4037              2_good actress_actress_actresses_cast   \n",
       "4       3    4005                3_movie movie_movie_film_good movie   \n",
       "5       4    3880  4_song trailer_trailer song_music trailer_soun...   \n",
       "6       5    2740              5_downvoted_right_yeah right_downvote   \n",
       "7       6    2480               6_choo choo_choo_fookin prawns_ho ho   \n",
       "8       7    2295            7_book book_second book_book_book movie   \n",
       "9       8    2166               8_shit pants_intensifies_pants_jeans   \n",
       "10      9    1932           9_release date_release_releases_released   \n",
       "11     10    1799                   10_vegan_gluten free_gluten_meat   \n",
       "12     11    1781  11_horror film_horror movie_horror films_horro...   \n",
       "13     12    1641      12_oscar bait_getting oscar_oscar baity_oscar   \n",
       "14     13    1463  13_british accent_english accent_british accen...   \n",
       "15     14    1401        14_black people_white people_blacks_racists   \n",
       "16     15    1246                         15_city_cities_filmed_town   \n",
       "17     16    1210                  16_money_money money_dollars_cash   \n",
       "18     17    1049  17_excited wait_looking forward_im excited_wai...   \n",
       "19     18    1048         18_harry potter_potter movies_potter_harry   \n",
       "20     19    1013             19_blue_orange blue_blue orange_colors   \n",
       "21     20     999       20_gonna watch_worth watching_watch_ll watch   \n",
       "22     21     951  21_second season_season season_seasons_episode...   \n",
       "23     22     936              22_cgi_better cgi_cgi really_just cgi   \n",
       "24     23     899            23_cast_cast cast_cast really_cast just   \n",
       "25     24     867                24_killer whales_shark_whales_whale   \n",
       "26     25     840        25_american sniper_sniper_soldier_war movie   \n",
       "27     26     805                 26_dog dog_dog_dog movie_dog scene   \n",
       "28     27     751                    27_gif oh_gif_gif gif_giphy gif   \n",
       "29     28     742  28_video games_video game_game movies_games movie   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [character, movie, characters, bad, film, does...   \n",
       "1   [teaser trailer, trailer trailer, watch traile...   \n",
       "2   [good actor, great actor, actor, director, act...   \n",
       "3   [good actress, actress, actresses, cast, actin...   \n",
       "4   [movie movie, movie, film, good movie, films, ...   \n",
       "5   [song trailer, trailer song, music trailer, so...   \n",
       "6   [downvoted, right, yeah right, downvote, downv...   \n",
       "7   [choo choo, choo, fookin prawns, ho ho, ho, pr...   \n",
       "8   [book book, second book, book, book movie, lov...   \n",
       "9   [shit pants, intensifies, pants, jeans, underw...   \n",
       "10  [release date, release, releases, released, ja...   \n",
       "11  [vegan, gluten free, gluten, meat, cheese, foo...   \n",
       "12  [horror film, horror movie, horror films, horr...   \n",
       "13  [oscar bait, getting oscar, oscar baity, oscar...   \n",
       "14  [british accent, english accent, british accen...   \n",
       "15  [black people, white people, blacks, racists, ...   \n",
       "16  [city, cities, filmed, town, downtown, movie f...   \n",
       "17  [money, money money, dollars, cash, make money...   \n",
       "18  [excited wait, looking forward, im excited, wa...   \n",
       "19  [harry potter, potter movies, potter, harry, h...   \n",
       "20  [blue, orange blue, blue orange, colors, colou...   \n",
       "21  [gonna watch, worth watching, watch, ll watch,...   \n",
       "22  [second season, season season, seasons, episod...   \n",
       "23  [cgi, better cgi, cgi really, just cgi, cgi ju...   \n",
       "24  [cast, cast cast, cast really, cast just, good...   \n",
       "25  [killer whales, shark, whales, whale, predator...   \n",
       "26  [american sniper, sniper, soldier, war movie, ...   \n",
       "27  [dog dog, dog, dog movie, dog scene, dog die, ...   \n",
       "28  [gif oh, gif, gif gif, giphy gif, gif just, co...   \n",
       "29  [video games, video game, game movies, games m...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [Have you not seen Celery Man?, It's not very ...  \n",
       "1   [Yeah, and what about \"Teaser trailer\" ?, Was ...  \n",
       "2   [Same director as Inside Out and Up so..., He'...  \n",
       "3   [Implying Twilight had a bad leading actress? ...  \n",
       "4   [In what movie?, I will see this movie ( ;) ) ...  \n",
       "5   [What is the song in the trailer?, song?, \\n[L...  \n",
       "6   [He's definitely being downvoted, that was the...  \n",
       "7   [Oh ho ho hohoho hoooooooooooooooooooooooooooo...  \n",
       "8   [The book!  You should read the book!, They sh...  \n",
       "9   [I just shit my pants. , Me too.  They make me...  \n",
       "10  [Release date?, AND STILL NO RELEASE DATE!?!?!...  \n",
       "11  [[**Cheese**](https://m.youtube.com/watch?v=oM...  \n",
       "12  [This is very obviously not a horror movie. , ...  \n",
       "13  [Not everything has to be oscar bait. , That a...  \n",
       "14  [+British accent+ \"Obviously\", I... liked his ...  \n",
       "15  [Because black people, Black people, Please pu...  \n",
       "16  [The city?, And a city., Filmed in Tampere! Ta...  \n",
       "17       [# TAKE MY MONEY!!!!!!!!!!!, Money, Money. ]  \n",
       "18  [Can't wait for this. , Now I can't wait!!, I ...  \n",
       "19  [*Fantastic Beasts and Where to Find Them: At ...  \n",
       "20  [Cinematography., Blue is the most human color...  \n",
       "21    [I must watch this., I’d watch., I'd watch it.]  \n",
       "22  [Six movies and a season., Where is season 2?,...  \n",
       "23           [Too much CGI now :(, Too much CGI, CGI]  \n",
       "24  [How would you of cast it?, What a cast., That...  \n",
       "25  [Predator Reddit. , Throwaway, because I spoke...  \n",
       "26  [The thing is that the \"heartland\" is anti-war...  \n",
       "27  [Yes? This is dog., The Dog and the Slumulous,...  \n",
       "28  [http://i.imgur.com/zaF6b0H.gif, http://i.imgu...  \n",
       "29  [For people that do not play video games. My w...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()[:30]#.to_csv(\"/Users/npop/Desktop/topic_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_documents(comments_full, reduced_embeddings=reduced_embeddings, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(topics=[0, 1, 2, 3, 4, 7, 17, 22], n_words=5, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(top_n_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate `X` and `labels` only for non-outlier topics (as they are technically not clusters)\n",
    "umap_embeddings = topic_model.umap_model.transform(embeddings)\n",
    "indices = [index for index, topic in enumerate(topics) if topic != -1]\n",
    "X = umap_embeddings[np.array(indices)]\n",
    "labels = [topic for index, topic in enumerate(topics) if topic != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6133194"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics and top words from BERTopic output\n",
    "topics = topic_model.get_topics()\n",
    "top_words = {topic: [word for word, _ in topic_model.get_topic(topic)] for topic in topics}\n",
    "\n",
    "# Preprocess top words (optional)\n",
    "# preprocess_top_words = preprocess(top_words)\n",
    "\n",
    "# Calculate coherence score for each topic\n",
    "coherence_scores = {}\n",
    "for topic_id, words in top_words.items():\n",
    "    cm = CoherenceModel(topics=[words], texts=documents, dictionary=dictionary, coherence='c_v') # Use 'c_v' or other coherence measures\n",
    "    coherence_score = cm.get_coherence()\n",
    "    coherence_scores[topic_id] = coherence_score\n",
    "\n",
    "# Average coherence score\n",
    "average_coherence = sum(coherence_scores.values()) / len(coherence_scores)\n",
    "print(\"Average Coherence Score:\", average_coherence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeetTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leet_df, topic_data = leet_topic.LeetTopic(reddit_comments,\n",
    "                                            document_field=\"body\",\n",
    "                                            html_filename=\"demo.html\",\n",
    "                                            extra_fields=[\"hdbscan_labels\"],\n",
    "                                            spacy_model=\"hr_core_news_sm\",\n",
    "                                            max_distance=.45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
