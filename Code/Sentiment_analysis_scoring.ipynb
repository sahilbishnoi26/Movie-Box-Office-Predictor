{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9eaf5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelWithLMHead\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72d12a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260781 entries, 0 to 260780\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   body              260781 non-null  object\n",
      " 1   post_date         260781 non-null  object\n",
      " 2   upvotes           260781 non-null  int64 \n",
      " 3   parent_id         194354 non-null  object\n",
      " 4   top_level_id      260781 non-null  object\n",
      " 5   post_title        260781 non-null  object\n",
      " 6   post_id           260781 non-null  object\n",
      " 7   post_create_date  260781 non-null  object\n",
      " 8   movie_searched    260781 non-null  object\n",
      " 9   release_date      260781 non-null  object\n",
      " 10  date_diff         260781 non-null  int64 \n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 21.9+ MB\n"
     ]
    }
   ],
   "source": [
    "reddit_comments_df=pd.read_csv('Datasets/reddit_comments_before_release.csv')\n",
    "reddit_comments_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f08c90ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39257</th>\n",
       "      <td>That's when they show you the deleted scenes.</td>\n",
       "      <td>That's when they show you the deleted scenes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216794</th>\n",
       "      <td>**Is there a** ***problem*** **gentlemen?**</td>\n",
       "      <td>**Is there a** ***problem*** **gentlemen?**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108583</th>\n",
       "      <td>Is that Robert Smigel? Mark Ruffalo? Oh, it's ...</td>\n",
       "      <td>Is that Robert Smigel? Mark Ruffalo? Oh, it's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>What the fuck was viper doing to her face?</td>\n",
       "      <td>What the fuck was viper doing to her face?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58157</th>\n",
       "      <td>Let me guess. A mean movie critic meets a sava...</td>\n",
       "      <td>Let me guess. A mean movie critic meets a sava...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body  \\\n",
       "39257       That's when they show you the deleted scenes.   \n",
       "216794        **Is there a** ***problem*** **gentlemen?**   \n",
       "108583  Is that Robert Smigel? Mark Ruffalo? Oh, it's ...   \n",
       "6835           What the fuck was viper doing to her face?   \n",
       "58157   Let me guess. A mean movie critic meets a sava...   \n",
       "\n",
       "                                             cleaned_body  \n",
       "39257       That's when they show you the deleted scenes.  \n",
       "216794        **Is there a** ***problem*** **gentlemen?**  \n",
       "108583  Is that Robert Smigel? Mark Ruffalo? Oh, it's ...  \n",
       "6835           What the fuck was viper doing to her face?  \n",
       "58157   Let me guess. A mean movie critic meets a sava...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_http_substrings(text):\n",
    "    # Pattern to match substrings starting with 'http' followed by any characters until the next space\n",
    "    pattern = r'http\\S+'\n",
    "    # Replace matched substrings with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "reddit_comments_df['cleaned_body'] = reddit_comments_df['body'].apply(remove_http_substrings)\n",
    "\n",
    "reddit_comments_df[['body','cleaned_body']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26bf192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replace non-alphanumeric characters (except for full stop) and newline characters with a space\n",
    "    text_with_spaces = re.sub(r'[^\\w\\.]|\\n', ' ', text)\n",
    "    # Collapse multiple adjacent spaces into a single space\n",
    "    cleaned_text = re.sub(r' +', ' ', text_with_spaces)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02898f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27121</th>\n",
       "      <td>Where black sheep at?</td>\n",
       "      <td>Where black sheep at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123203</th>\n",
       "      <td>Who knows? \\n\\nI felt it from the first time I...</td>\n",
       "      <td>Who knows I felt it from the first time I saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122137</th>\n",
       "      <td>No Franchise is safe</td>\n",
       "      <td>No Franchise is safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220509</th>\n",
       "      <td>Would be one if A24 actually started an awards...</td>\n",
       "      <td>Would be one if A24 actually started an awards...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208945</th>\n",
       "      <td>He was in the second one. (though they probabl...</td>\n",
       "      <td>He was in the second one. though they probably...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body  \\\n",
       "27121                               Where black sheep at?   \n",
       "123203  Who knows? \\n\\nI felt it from the first time I...   \n",
       "122137                               No Franchise is safe   \n",
       "220509  Would be one if A24 actually started an awards...   \n",
       "208945  He was in the second one. (though they probabl...   \n",
       "\n",
       "                                             cleaned_body  \n",
       "27121                               Where black sheep at   \n",
       "123203  Who knows I felt it from the first time I saw ...  \n",
       "122137                               No Franchise is safe  \n",
       "220509  Would be one if A24 actually started an awards...  \n",
       "208945  He was in the second one. though they probably...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_comments_df['cleaned_body'] = reddit_comments_df['cleaned_body'].apply(clean_text)\n",
    "\n",
    "reddit_comments_df[['body','cleaned_body']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "349e27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n",
    "\n",
    "#model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3e5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
    "    output = model.generate(input_ids=input_ids,max_length=2)\n",
    "    dec = [tokenizer.decode(ids) for ids in output]\n",
    "    #print(dec)\n",
    "    label = dec[0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fbf8462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "#sample =get_sentiment(\"I dislike a lot that film\")\n",
    "#print(sample.split(' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa82bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#print(type(get_sentiment(\"I dislike a lot that film\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit_comments_df['sentiment_using_t5_imdb'] = reddit_comments_df.apply(lambda x: get_sentiment(x['body']).split(' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd49713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(comments_batch):\n",
    "    # Encode all comments in the batch\n",
    "    input_ids = tokenizer.batch_encode_plus(comments_batch + ['</s>'], return_tensors='pt', padding=True, truncation=True, max_length=512)['input_ids']\n",
    "    # Generate output for the entire batch\n",
    "    output = model.generate(input_ids=input_ids, max_length=2)\n",
    "    # Decode predictions\n",
    "    labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output[:-1]]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "decc7d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1000  # Adjust based on your system's capability\n",
    "num_batches = (len(reddit_comments_df) + batch_size - 1) // batch_size  # Calculate how many batches are needed\n",
    "\n",
    "#num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    # Select the batch of comments\n",
    "    comments_batch = reddit_comments_df.iloc[start_idx:end_idx]['body'].tolist()\n",
    "    # Process the batch\n",
    "    batch_labels = process_batch(comments_batch)\n",
    "    # Store the results\n",
    "    reddit_comments_df.loc[start_idx:end_idx-1, 'sentiment_using_t5_imdb'] = batch_labels\n",
    "    # Optional: Save progress intermittently\n",
    "    if i % 10 == 0:  # Save every 10 batches, adjust based on your preference\n",
    "        reddit_comments_df.to_csv('partial_sentiment_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1052a3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_batch(reddit_comments_df.iloc[0:10]['cleaned_body'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bed0d16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Damn even Scorsese pirated Yeezus.', ' Leo loves it though ', ' Not as much as Ethan Suplee ', 'Please put white people over this.', ' close up of dat face ', 'I can watch this allll day.', 'Is that Ethan Suplee in the background EDIT I think it may be ', ' awwww yeah it is ', 'Just watched the trailer went to this thread on this sub specifically to see if anyone had made a gif of that bit yet. You never disappoint me reddit', 'Is that Jon Bernthal in the background ']\n"
     ]
    }
   ],
   "source": [
    "#print(reddit_comments_df.iloc[0:10]['cleaned_body'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee0895a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a59b2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL2 = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(MODEL2)\n",
    "config2 = AutoConfig.from_pretrained(MODEL2)\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(MODEL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b70e0a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid cases are increasing fast!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.72357637, 0.22867927, 0.04774434], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# PT\n",
    "\n",
    "#model.save_pretrained(MODEL)\n",
    "text = \"Covid cases are increasing fast!\"\n",
    "text = preprocess(text)\n",
    "print(text)\n",
    "encoded_input = tokenizer2(text, return_tensors='pt')\n",
    "output = model2(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb83ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "3\n",
      "negative\n",
      "neutral\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "print(ranking)\n",
    "print(scores.shape[0])\n",
    "for i in range(scores.shape[0]):\n",
    "    print(config2.id2label[ranking[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b5e4bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative', 1: 'neutral', 2: 'positive'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc10c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sentiment_analysis(comments, batch_size=32):\n",
    "    \"\"\"\n",
    "    Process comments in batches and return sentiment scores.\n",
    "\n",
    "    Parameters:\n",
    "    - comments (list of str): The comments to analyze.\n",
    "    - batch_size (int): The size of batches to process at a time.\n",
    "\n",
    "    Returns:\n",
    "    - List of dicts with sentiment scores for each comment.\n",
    "    \"\"\"\n",
    "    # Divide comments into batches\n",
    "    batches = [comments[i:i + batch_size] for i in range(0, len(comments), batch_size)]\n",
    "    sentiment_scores = []\n",
    "\n",
    "    for batch in batches:\n",
    "        # Encode batch\n",
    "        encoded_input = tokenizer2(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            output = model2(**encoded_input)\n",
    "        # Apply softmax to get probabilities\n",
    "        scores = softmax(output.logits.detach().numpy(), axis=1)\n",
    "        \n",
    "        # Store scores\n",
    "        for score in scores:\n",
    "            sentiment_scores.append({'negative': score[0], 'neutral': score[1], 'positive': score[2]})\n",
    "    \n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87fd5ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'negative': 0.72357637, 'neutral': 0.22867927, 'positive': 0.047744345}, {'negative': 0.0034665263, 'neutral': 0.008983754, 'positive': 0.9875497}, {'negative': 0.6891944, 'neutral': 0.30030933, 'positive': 0.010496268}]\n"
     ]
    }
   ],
   "source": [
    "comments = [\n",
    "    \"Covid cases are increasing fast!\",\n",
    "    \"The new movie is fantastic!\",\n",
    "    \"I'm not sure how I feel about the latest policy changes.\"\n",
    "]\n",
    "sentiment_scores = batch_sentiment_analysis(comments)\n",
    "print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c066075",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "61faae2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 260000-269999 processed and saved to sentiment_scores_260000_269999.csv. Processing time: 101.36 seconds.\n",
      "All batches combined and saved to combined_sentiment_scores_all1.csv.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for start_idx in range(260000, 270000, 10000):  # Adjust the range as needed\n",
    "    start_time = time.time()\n",
    "    end_idx = start_idx + 10000\n",
    "    # Select subset of rows for the current batch\n",
    "    subset_comments = reddit_comments_df1['cleaned_body'].iloc[start_idx:end_idx].tolist()\n",
    "    # Perform sentiment analysis on the current batch\n",
    "    sentiment_scores = batch_sentiment_analysis(subset_comments)\n",
    "    # Convert sentiment scores to DataFrame\n",
    "    batch_df = pd.json_normalize(sentiment_scores)\n",
    "    # Optional: Combine with original comments if needed\n",
    "    batch_df['comments'] = subset_comments\n",
    "    \n",
    "    sentiment_df = pd.concat([sentiment_df, batch_df], ignore_index=True)\n",
    "    # Save to CSV with a filename reflecting the index range\n",
    "    filename = f\"sentiment_scores_{start_idx}_{end_idx-1}.csv\"\n",
    "    batch_df.to_csv(filename, index=False)\n",
    "    end_time = time.time()  # Capture end time\n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Batch {start_idx}-{end_idx-1} processed and saved to {filename}. Processing time: {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "combined_df_filename = \"combined_sentiment_scores_all1.csv\"\n",
    "sentiment_df.to_csv(combined_df_filename, index=False)\n",
    "print(f\"All batches combined and saved to {combined_df_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e53a8881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260781 entries, 0 to 260780\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   negative  260781 non-null  float32\n",
      " 1   neutral   260781 non-null  float32\n",
      " 2   positive  260781 non-null  float32\n",
      " 3   comments  260781 non-null  object \n",
      "dtypes: float32(3), object(1)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "546ce0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260781 entries, 0 to 260780\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   body              260781 non-null  object\n",
      " 1   post_date         260781 non-null  object\n",
      " 2   upvotes           260781 non-null  int64 \n",
      " 3   parent_id         194354 non-null  object\n",
      " 4   top_level_id      260781 non-null  object\n",
      " 5   post_title        260781 non-null  object\n",
      " 6   post_id           260781 non-null  object\n",
      " 7   post_create_date  260781 non-null  object\n",
      " 8   movie_searched    260781 non-null  object\n",
      " 9   release_date      260781 non-null  object\n",
      " 10  date_diff         260781 non-null  int64 \n",
      " 11  cleaned_body      260781 non-null  object\n",
      "dtypes: int64(2), object(10)\n",
      "memory usage: 23.9+ MB\n"
     ]
    }
   ],
   "source": [
    "reddit_comments_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce24bbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>post_date</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>top_level_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_create_date</th>\n",
       "      <th>movie_searched</th>\n",
       "      <th>release_date</th>\n",
       "      <th>date_diff</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36151</th>\n",
       "      <td>He sacrificed himself so the others could get ...</td>\n",
       "      <td>2014-05-08 16:36:07</td>\n",
       "      <td>2</td>\n",
       "      <td>chcplsg</td>\n",
       "      <td>chcplsg</td>\n",
       "      <td>New full-length trailer for 'Dawn of the Plane...</td>\n",
       "      <td>251d6x</td>\n",
       "      <td>2014-05-08 12:58:58</td>\n",
       "      <td>Dawn of the Planet of the Apes</td>\n",
       "      <td>2014-07-11</td>\n",
       "      <td>63</td>\n",
       "      <td>He sacrificed himself so the others could get ...</td>\n",
       "      <td>0.724227</td>\n",
       "      <td>0.257040</td>\n",
       "      <td>0.018733</td>\n",
       "      <td>He sacrificed himself so the others could get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205121</th>\n",
       "      <td>Well it certainly is a pretty grim tale</td>\n",
       "      <td>2019-09-04 19:23:44</td>\n",
       "      <td>2</td>\n",
       "      <td>eyzjetx</td>\n",
       "      <td>eyz1zhc</td>\n",
       "      <td>GRETEL &amp; HANSEL Official Teaser Trailer (2020)</td>\n",
       "      <td>czkm9i</td>\n",
       "      <td>2019-09-04 13:06:18</td>\n",
       "      <td>Gretel &amp; Hansel</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>148</td>\n",
       "      <td>Well it certainly is a pretty grim tale</td>\n",
       "      <td>0.901787</td>\n",
       "      <td>0.089228</td>\n",
       "      <td>0.008985</td>\n",
       "      <td>Well it certainly is a pretty grim tale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155729</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2018-06-09 14:14:35</td>\n",
       "      <td>5</td>\n",
       "      <td>e0dd7eu</td>\n",
       "      <td>e0cy5go</td>\n",
       "      <td>First Man (Official Trailer)</td>\n",
       "      <td>8pp1f6</td>\n",
       "      <td>2018-06-09 00:31:14</td>\n",
       "      <td>First Man</td>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>124</td>\n",
       "      <td>deleted</td>\n",
       "      <td>0.138816</td>\n",
       "      <td>0.801894</td>\n",
       "      <td>0.059289</td>\n",
       "      <td>deleted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127333</th>\n",
       "      <td>It was a great \"haunted house\" type movie beca...</td>\n",
       "      <td>2016-12-25 06:54:43</td>\n",
       "      <td>542</td>\n",
       "      <td>dblutpc</td>\n",
       "      <td>dblso9k</td>\n",
       "      <td>Alien: Covenant - Official Trailer</td>\n",
       "      <td>5k6yfp</td>\n",
       "      <td>2016-12-25 05:00:11</td>\n",
       "      <td>Alien: Covenant</td>\n",
       "      <td>2017-05-19</td>\n",
       "      <td>144</td>\n",
       "      <td>It was a great haunted house type movie becaus...</td>\n",
       "      <td>0.434173</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>0.335524</td>\n",
       "      <td>It was a great haunted house type movie becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135099</th>\n",
       "      <td>I think he knew he could ride that money train...</td>\n",
       "      <td>2017-05-29 03:16:34</td>\n",
       "      <td>8</td>\n",
       "      <td>di66q5e</td>\n",
       "      <td>di5wmeh</td>\n",
       "      <td>LOGAN LUCKY | Official Trailer</td>\n",
       "      <td>6dwetc</td>\n",
       "      <td>2017-05-28 21:51:26</td>\n",
       "      <td>Logan Lucky</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>80</td>\n",
       "      <td>I think he knew he could ride that money train...</td>\n",
       "      <td>0.064902</td>\n",
       "      <td>0.716132</td>\n",
       "      <td>0.218966</td>\n",
       "      <td>I think he knew he could ride that money train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48022</th>\n",
       "      <td>;) you welcome.</td>\n",
       "      <td>2021-06-09 18:48:42</td>\n",
       "      <td>1</td>\n",
       "      <td>h16owxv</td>\n",
       "      <td>h15mva4</td>\n",
       "      <td>THE EYES OF TAMMY FAYE | Official Trailer | Se...</td>\n",
       "      <td>nvve49</td>\n",
       "      <td>2021-06-09 13:01:14</td>\n",
       "      <td>The Eyes of Tammy Faye</td>\n",
       "      <td>2021-09-17</td>\n",
       "      <td>99</td>\n",
       "      <td>you welcome.</td>\n",
       "      <td>0.022014</td>\n",
       "      <td>0.187035</td>\n",
       "      <td>0.790951</td>\n",
       "      <td>you welcome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85770</th>\n",
       "      <td>See the transition from joy to dread, he must ...</td>\n",
       "      <td>2015-12-31 19:13:26</td>\n",
       "      <td>16</td>\n",
       "      <td>cyhjw7y</td>\n",
       "      <td>cyh9pjs</td>\n",
       "      <td>'Zootopia' Official US Trailer #2</td>\n",
       "      <td>3ywslq</td>\n",
       "      <td>2015-12-31 13:45:01</td>\n",
       "      <td>Zootopia</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>63</td>\n",
       "      <td>See the transition from joy to dread he must h...</td>\n",
       "      <td>0.529291</td>\n",
       "      <td>0.393967</td>\n",
       "      <td>0.076742</td>\n",
       "      <td>See the transition from joy to dread he must h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137007</th>\n",
       "      <td>No, it's more than that. The ghost's existenc...</td>\n",
       "      <td>2017-03-29 00:55:54</td>\n",
       "      <td>4</td>\n",
       "      <td>dfioy15</td>\n",
       "      <td>dfiefvh</td>\n",
       "      <td>A Ghost Story | Official Trailer HD | A24 (Cas...</td>\n",
       "      <td>61z6vm</td>\n",
       "      <td>2017-03-28 13:02:03</td>\n",
       "      <td>A Ghost Story</td>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>99</td>\n",
       "      <td>No it s more than that. The ghost s existence...</td>\n",
       "      <td>0.257965</td>\n",
       "      <td>0.703821</td>\n",
       "      <td>0.038215</td>\n",
       "      <td>No it s more than that. The ghost s existence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203611</th>\n",
       "      <td>Julie Kavner does Marge's voice on The Simpson...</td>\n",
       "      <td>2019-11-14 20:13:48</td>\n",
       "      <td>11</td>\n",
       "      <td>f7i7b9e</td>\n",
       "      <td>f7hmiw4</td>\n",
       "      <td>The SpongeBob Movie: Sponge on the Run (2020) ...</td>\n",
       "      <td>dw9wj6</td>\n",
       "      <td>2019-11-14 14:00:16</td>\n",
       "      <td>The SpongeBob Movie: Sponge on the Run</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>475</td>\n",
       "      <td>Julie Kavner does Marge s voice on The Simpson...</td>\n",
       "      <td>0.019643</td>\n",
       "      <td>0.637103</td>\n",
       "      <td>0.343254</td>\n",
       "      <td>Julie Kavner does Marge s voice on The Simpson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>oh gods please make more Incredibles movies</td>\n",
       "      <td>2012-06-20 11:58:20</td>\n",
       "      <td>17</td>\n",
       "      <td>c531s1f</td>\n",
       "      <td>c530zko</td>\n",
       "      <td>Monsters inc. sequal, Monsters University trai...</td>\n",
       "      <td>vbimw</td>\n",
       "      <td>2012-06-20 07:44:34</td>\n",
       "      <td>Monsters University</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>365</td>\n",
       "      <td>oh gods please make more Incredibles movies</td>\n",
       "      <td>0.029964</td>\n",
       "      <td>0.113917</td>\n",
       "      <td>0.856119</td>\n",
       "      <td>oh gods please make more Incredibles movies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body  \\\n",
       "36151   He sacrificed himself so the others could get ...   \n",
       "205121            Well it certainly is a pretty grim tale   \n",
       "155729                                          [deleted]   \n",
       "127333  It was a great \"haunted house\" type movie beca...   \n",
       "135099  I think he knew he could ride that money train...   \n",
       "48022                                     ;) you welcome.   \n",
       "85770   See the transition from joy to dread, he must ...   \n",
       "137007   No, it's more than that. The ghost's existenc...   \n",
       "203611  Julie Kavner does Marge's voice on The Simpson...   \n",
       "5978          oh gods please make more Incredibles movies   \n",
       "\n",
       "                  post_date  upvotes parent_id top_level_id  \\\n",
       "36151   2014-05-08 16:36:07        2   chcplsg      chcplsg   \n",
       "205121  2019-09-04 19:23:44        2   eyzjetx      eyz1zhc   \n",
       "155729  2018-06-09 14:14:35        5   e0dd7eu      e0cy5go   \n",
       "127333  2016-12-25 06:54:43      542   dblutpc      dblso9k   \n",
       "135099  2017-05-29 03:16:34        8   di66q5e      di5wmeh   \n",
       "48022   2021-06-09 18:48:42        1   h16owxv      h15mva4   \n",
       "85770   2015-12-31 19:13:26       16   cyhjw7y      cyh9pjs   \n",
       "137007  2017-03-29 00:55:54        4   dfioy15      dfiefvh   \n",
       "203611  2019-11-14 20:13:48       11   f7i7b9e      f7hmiw4   \n",
       "5978    2012-06-20 11:58:20       17   c531s1f      c530zko   \n",
       "\n",
       "                                               post_title post_id  \\\n",
       "36151   New full-length trailer for 'Dawn of the Plane...  251d6x   \n",
       "205121     GRETEL & HANSEL Official Teaser Trailer (2020)  czkm9i   \n",
       "155729                       First Man (Official Trailer)  8pp1f6   \n",
       "127333                 Alien: Covenant - Official Trailer  5k6yfp   \n",
       "135099                     LOGAN LUCKY | Official Trailer  6dwetc   \n",
       "48022   THE EYES OF TAMMY FAYE | Official Trailer | Se...  nvve49   \n",
       "85770                   'Zootopia' Official US Trailer #2  3ywslq   \n",
       "137007  A Ghost Story | Official Trailer HD | A24 (Cas...  61z6vm   \n",
       "203611  The SpongeBob Movie: Sponge on the Run (2020) ...  dw9wj6   \n",
       "5978    Monsters inc. sequal, Monsters University trai...   vbimw   \n",
       "\n",
       "           post_create_date                          movie_searched  \\\n",
       "36151   2014-05-08 12:58:58          Dawn of the Planet of the Apes   \n",
       "205121  2019-09-04 13:06:18                         Gretel & Hansel   \n",
       "155729  2018-06-09 00:31:14                               First Man   \n",
       "127333  2016-12-25 05:00:11                         Alien: Covenant   \n",
       "135099  2017-05-28 21:51:26                             Logan Lucky   \n",
       "48022   2021-06-09 13:01:14                  The Eyes of Tammy Faye   \n",
       "85770   2015-12-31 13:45:01                                Zootopia   \n",
       "137007  2017-03-28 13:02:03                           A Ghost Story   \n",
       "203611  2019-11-14 14:00:16  The SpongeBob Movie: Sponge on the Run   \n",
       "5978    2012-06-20 07:44:34                     Monsters University   \n",
       "\n",
       "       release_date  date_diff  \\\n",
       "36151    2014-07-11         63   \n",
       "205121   2020-01-31        148   \n",
       "155729   2018-10-12        124   \n",
       "127333   2017-05-19        144   \n",
       "135099   2017-08-18         80   \n",
       "48022    2021-09-17         99   \n",
       "85770    2016-03-04         63   \n",
       "137007   2017-07-07         99   \n",
       "203611   2021-03-04        475   \n",
       "5978     2013-06-21        365   \n",
       "\n",
       "                                             cleaned_body  negative   neutral  \\\n",
       "36151   He sacrificed himself so the others could get ...  0.724227  0.257040   \n",
       "205121            Well it certainly is a pretty grim tale  0.901787  0.089228   \n",
       "155729                                           deleted   0.138816  0.801894   \n",
       "127333  It was a great haunted house type movie becaus...  0.434173  0.230303   \n",
       "135099  I think he knew he could ride that money train...  0.064902  0.716132   \n",
       "48022                                        you welcome.  0.022014  0.187035   \n",
       "85770   See the transition from joy to dread he must h...  0.529291  0.393967   \n",
       "137007   No it s more than that. The ghost s existence...  0.257965  0.703821   \n",
       "203611  Julie Kavner does Marge s voice on The Simpson...  0.019643  0.637103   \n",
       "5978          oh gods please make more Incredibles movies  0.029964  0.113917   \n",
       "\n",
       "        positive                                           comments  \n",
       "36151   0.018733  He sacrificed himself so the others could get ...  \n",
       "205121  0.008985            Well it certainly is a pretty grim tale  \n",
       "155729  0.059289                                           deleted   \n",
       "127333  0.335524  It was a great haunted house type movie becaus...  \n",
       "135099  0.218966  I think he knew he could ride that money train...  \n",
       "48022   0.790951                                       you welcome.  \n",
       "85770   0.076742  See the transition from joy to dread he must h...  \n",
       "137007  0.038215   No it s more than that. The ghost s existence...  \n",
       "203611  0.343254  Julie Kavner does Marge s voice on The Simpson...  \n",
       "5978    0.856119        oh gods please make more Incredibles movies  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_merge = reddit_comments_df1.merge(sentiment_df, left_index=True, right_index=True)\n",
    "sentiment_merge.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e6c2b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_merge.drop(['comments'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "591e517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 252228 entries, 0 to 260780\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   body              252228 non-null  object \n",
      " 1   post_date         252228 non-null  object \n",
      " 2   upvotes           252228 non-null  int64  \n",
      " 3   parent_id         187539 non-null  object \n",
      " 4   top_level_id      252228 non-null  object \n",
      " 5   post_title        252228 non-null  object \n",
      " 6   post_id           252228 non-null  object \n",
      " 7   post_create_date  252228 non-null  object \n",
      " 8   movie_searched    252228 non-null  object \n",
      " 9   release_date      252228 non-null  object \n",
      " 10  date_diff         252228 non-null  int64  \n",
      " 11  cleaned_body      252228 non-null  object \n",
      " 12  negative          252228 non-null  float32\n",
      " 13  neutral           252228 non-null  float32\n",
      " 14  positive          252228 non-null  float32\n",
      "dtypes: float32(3), int64(2), object(10)\n",
      "memory usage: 27.9+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_df = sentiment_merge[~sentiment_merge['body'].str.contains('\\[deleted\\]|\\[removed\\]', na=False)]\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da739b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 251097 entries, 0 to 260780\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   body              251097 non-null  object \n",
      " 1   post_date         251097 non-null  object \n",
      " 2   upvotes           251097 non-null  int64  \n",
      " 3   parent_id         186502 non-null  object \n",
      " 4   top_level_id      251097 non-null  object \n",
      " 5   post_title        251097 non-null  object \n",
      " 6   post_id           251097 non-null  object \n",
      " 7   post_create_date  251097 non-null  object \n",
      " 8   movie_searched    251097 non-null  object \n",
      " 9   release_date      251097 non-null  object \n",
      " 10  date_diff         251097 non-null  int64  \n",
      " 11  cleaned_body      251097 non-null  object \n",
      " 12  negative          251097 non-null  float32\n",
      " 13  neutral           251097 non-null  float32\n",
      " 14  positive          251097 non-null  float32\n",
      "dtypes: float32(3), int64(2), object(10)\n",
      "memory usage: 27.8+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_df = filtered_df[filtered_df['cleaned_body'].str.strip() != '']\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c765a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"reddit_comments_sentiment_scores_hugging_face(twitter).csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24b1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
